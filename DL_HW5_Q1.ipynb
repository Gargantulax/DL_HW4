{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit-pypi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcLaGaTLHSkq",
        "outputId": "57adfd8d-d927-4b99-d3c1-7ced5919c2e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (9.4.0)\n",
            "Installing collected packages: rdkit-pypi\n",
            "Successfully installed rdkit-pypi-2022.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from rdkit import Chem\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import warnings"
      ],
      "metadata": {
        "id": "cTScbe286vhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MihQOV4_uqyA",
        "outputId": "7894b4fe-3fff-4539-eeca-3b2c6a4df68a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc2HmD8NxKsL"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('BBBP.csv')\n",
        "\n",
        "# Calculate chain lengths\n",
        "df['smiles_length'] = df['smiles'].apply(lambda x: len(x))\n",
        "\n",
        "atoms = []\n",
        "for molecule in df['smiles']:\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(molecule)\n",
        "        for atom in mol.GetAtoms():\n",
        "            atoms.append(atom.GetSymbol())\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "unique_atoms = set(atoms)\n",
        "\n",
        "# Count the frequency of each token\n",
        "token_counts = Counter(atoms)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a report\n",
        "report = f\"\"\"\n",
        "### SMILES Format:\n",
        "SMILES (Simplified Molecular Input Line Entry System) is a notation that allows a user to represent a chemical structure in a way that can be used by the computer. It consists of a series of symbols and characters representing the structure of a molecule.\n",
        "\n",
        "### Chain Length:\n",
        "Chain length refers to the length of the molecular structure represented in the SMILES format. In this context, we can interpret it as the number of characters in the SMILES string.\n",
        "\n",
        "#### Chain Length Distribution:\n",
        "{df['smiles_length'].describe()}\n",
        "\n",
        "### Token Variety and Frequency:\n",
        "Tokens are individual units (characters or sub-sequences) that make up the SMILES strings. Analyzing the variety and frequency of tokens can provide insights into the diversity of molecular structures in the dataset.\n",
        "\n",
        "#### Token Variety:\n",
        "{len(token_counts)} unique tokens\n",
        "\n",
        "#### Token Frequency:\n",
        "{token_counts.most_common()}\n",
        "\"\"\"\n",
        "\n",
        "print(report)"
      ],
      "metadata": {
        "id": "MMGtawJ6BqWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7722fc61-b2b8-4c31-8509-7c13b38d910d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### SMILES Format:\n",
            "SMILES (Simplified Molecular Input Line Entry System) is a notation that allows a user to represent a chemical structure in a way that can be used by the computer. It consists of a series of symbols and characters representing the structure of a molecule.\n",
            "\n",
            "### Chain Length:\n",
            "Chain length refers to the length of the molecular structure represented in the SMILES format. In this context, we can interpret it as the number of characters in the SMILES string.\n",
            "\n",
            "#### Chain Length Distribution:\n",
            "count    2050.000000\n",
            "mean       51.474146\n",
            "std        30.620659\n",
            "min         3.000000\n",
            "25%        33.000000\n",
            "50%        45.000000\n",
            "75%        61.000000\n",
            "max       400.000000\n",
            "Name: smiles_length, dtype: float64\n",
            "\n",
            "### Token Variety and Frequency:\n",
            "Tokens are individual units (characters or sub-sequences) that make up the SMILES strings. Analyzing the variety and frequency of tokens can provide insights into the diversity of molecular structures in the dataset.\n",
            "\n",
            "#### Token Variety:\n",
            "13 unique tokens\n",
            "\n",
            "#### Token Frequency:\n",
            "[('C', 36596), ('O', 6254), ('N', 4391), ('Cl', 618), ('S', 570), ('F', 509), ('Br', 50), ('H', 40), ('Na', 21), ('P', 11), ('I', 6), ('Ca', 1), ('B', 1)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_smiles = ''.join(df['smiles'].tolist())\n",
        "unique_syms_nums = ''.join(char for char in all_smiles if not char.isalpha())\n",
        "unique_tokens = set(unique_atoms)\n",
        "unique_tokens.update(set(unique_syms_nums))\n",
        "unique_tokens = list(unique_tokens) + ['c', 'n', 'o', 's']\n",
        "print(unique_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXSrb5wr_z_T",
        "outputId": "6cc616f1-c662-4a2a-9538-2286a0126c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['F', '%', '0', 'N', 'O', '9', '4', 'H', ']', '(', 'P', 'B', '=', '6', '7', 'Br', 'Ca', '[', 'Cl', '2', '.', 'C', '#', '8', '/', '@', '3', ')', 'S', '-', '5', '1', 'I', '\\\\', '+', 'Na', 'c', 'n', 'o', 's']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode tokens using one-hot encoding\n",
        "def smiles_to_onehot(molecule, token_index, code_size):\n",
        "    molecule_tokens = []\n",
        "    current_element = ''\n",
        "    i = 0\n",
        "    while i < len(molecule):\n",
        "        current_element += molecule[i]\n",
        "        while current_element in unique_tokens:\n",
        "            i += 1\n",
        "            if i < len(molecule):\n",
        "                current_element += molecule[i]\n",
        "            else:\n",
        "                current_element += 'T'\n",
        "        molecule_tokens.append(current_element[:-1])\n",
        "        current_element = ''\n",
        "\n",
        "    result = torch.zeros(len(molecule_tokens), code_size)\n",
        "    for i, token in enumerate(molecule_tokens):\n",
        "        onehot = torch.zeros(code_size)\n",
        "        onehot[token_index[token]] = 1\n",
        "        result[i,:] = onehot\n",
        "    return result\n",
        "\n",
        "# Convert labels to PyTorch tensor\n",
        "labels = torch.tensor(df['p_np'].values, dtype=torch.float32)\n",
        "\n",
        "# Create token index\n",
        "token_index = {token: i for i, token in enumerate(unique_tokens)}\n",
        "\n",
        "# Encode SMILES strings with padding\n",
        "encoded_smiles = [smiles_to_onehot(molecule, token_index, len(token_index)) for molecule in df['smiles']]\n",
        "\n",
        "max_tokens_in_molecule = max(len(tensor) for tensor in encoded_smiles)\n",
        "# Zero-pad each tensor to make them all have the same length of token_index\n",
        "encoded_smiles = [torch.nn.functional.pad(tensor, (0, 0, 0, max_tokens_in_molecule - len(tensor))) for tensor in encoded_smiles]\n",
        "\n",
        "# Stack the padded tensors to create the final tensor\n",
        "encoded_smiles = torch.stack(encoded_smiles)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(encoded_smiles, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "LhinNRI3KE9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the FC neural network\n",
        "class FCNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(FCNet, self).__init__()\n",
        "        self.flat = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flat(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = X_train.shape[1] * X_train.shape[2]\n",
        "hidden_size = 128\n",
        "output_size = 1\n",
        "model = FCNet(input_size, hidden_size, output_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "epochs = 50\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model on test data\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_outputs = model(X_test)\n",
        "    predictions = (test_outputs.squeeze() > 0.5).float()\n",
        "    accuracy = (predictions == y_test).float().mean()\n",
        "\n",
        "print(f\"\\n Accuracy on test data: {accuracy.item() * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy-X1LV47RYt",
        "outputId": "78d51926-7965-433a-85ff-febccfa3d639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:57<00:00,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Accuracy on test data: 86.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM + FC neural network\n",
        "class MolecularLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, lstm_layers, fc_hidden_size, output_size):\n",
        "        super(MolecularLSTM, self).__init__()\n",
        "        self.num_layers = lstm_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, 10, num_layers=lstm_layers, batch_first=True)\n",
        "        self.flat = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(10*hidden_size, fc_hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.bn = nn.BatchNorm1d(fc_hidden_size)\n",
        "        self.fc2 = nn.Linear(fc_hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden_states = torch.zeros(self.num_layers, x.size(0), 10)\n",
        "        cell_states = torch.zeros(self.num_layers, x.size(0), 10)\n",
        "        x, _ = self.lstm(x, (hidden_states, cell_states))\n",
        "        x = self.flat(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(self.bn(x))\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = X_train.shape[2]\n",
        "hidden_size = X_train.shape[1]\n",
        "lstm_layers = 1\n",
        "fc_hidden_size = 64\n",
        "output_size = 1\n",
        "\n",
        "model = MolecularLSTM(input_size, hidden_size, lstm_layers, fc_hidden_size, output_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "epochs = 100\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model on test data\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_outputs = model(X_test)\n",
        "    predictions = (test_outputs.squeeze() > 0.5).float()\n",
        "    accuracy = (predictions == y_test).float().mean()\n",
        "\n",
        "print(f\"\\n Accuracy on test data: {accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT6DrsO2AcOo",
        "outputId": "4083bb2c-f50f-45ee-f71d-518bc2d8bf31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [03:11<00:00,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Accuracy on test data: 83.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the BiLSTM + FC neural network\n",
        "class MolecularBiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, lstm_layers, fc_hidden_size, output_size):\n",
        "        super(MolecularLSTM, self).__init__()\n",
        "        self.num_layers = lstm_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bilstm = nn.LSTM(input_size, 10, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
        "        self.flat = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(10*hidden_size*2, fc_hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.bn = nn.BatchNorm1d(fc_hidden_size)\n",
        "        self.fc2 = nn.Linear(fc_hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden_states = torch.zeros(self.num_layers, x.size(0), 10)\n",
        "        cell_states = torch.zeros(self.num_layers, x.size(0), 10)\n",
        "        x, _ = self.lstm(x, (hidden_states, cell_states))\n",
        "        x = self.flat(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(self.bn(x))\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = X_train.shape[2]\n",
        "hidden_size = X_train.shape[1]\n",
        "lstm_layers = 1\n",
        "fc_hidden_size = 64\n",
        "output_size = 1\n",
        "\n",
        "model = MolecularLSTM(input_size, hidden_size, lstm_layers, fc_hidden_size, output_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "epochs = 100\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model on test data\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_outputs = model(X_test)\n",
        "    predictions = (test_outputs.squeeze() > 0.5).float()\n",
        "    accuracy = (predictions == y_test).float().mean()\n",
        "\n",
        "print(f\"\\n Accuracy on test data: {accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2jT_C0xtQ13",
        "outputId": "d2ab3f0d-6f99-49d5-ec8a-ef04163dc837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [03:11<00:00,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Accuracy on test data: 82.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}